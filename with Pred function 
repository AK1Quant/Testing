# Load necessary libraries
library(xgboost)
library(dplyr)
library(tibble)
library(readxl)
library(zoo)
library(ParBayesianOptimization)
library(caret)
library(Metrics)

## DATA

# Load the US VIX data
vix_data <- read_excel("VIX_History.xlsx")

# Convert date column to date format and sort the data
vix_data$DATE <- as.Date(vix_data$DATE)

# Feature engineering: create lag features and moving averages
vix_data <- vix_data %>%
  mutate(
    lag1 = lag(VIX, n = 1),
    lag2 = lag(VIX, n = 2),
    lag3 = lag(VIX, n = 3),
    ma3 = rollmean(VIX, k = 3, fill = NA, align = 'right'),
    ma5 = rollmean(VIX, k = 5, fill = NA, align = 'right')
  )

# Remove rows with NA values
vix_data <- vix_data %>%
  na.omit()

## Splitting the Data

# Split data into training and testing sets (80-20 split)
split_point <- floor(0.8 * nrow(vix_data))
train_data <- vix_data[1:split_point, ]
test_data <- vix_data[(split_point + 1):nrow(vix_data), ]

# Convert data to DMatrix format
train_matrix <- as.matrix(train_data[, -1])  # Exclude Date column
train_label <- train_data$VIX
train_dmatrix <- xgb.DMatrix(data = train_matrix, label = train_label)

test_matrix <- as.matrix(test_data[, -1])
test_label <- test_data$VIX
test_dmatrix <- xgb.DMatrix(data = test_matrix, label = test_label)

# Bayesian optimization process

xgboost_optimization <- function(max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample) {
    # Define the parameters for the model
    params <- list(
        booster = "gbtree",
        objective = "reg:squarederror",  # Specify the regression objective
        max_depth = as.integer(max_depth),
        eta = eta,
        gamma = gamma,
        colsample_bytree = colsample_bytree,
        min_child_weight = min_child_weight,
        subsample = subsample
    )
    
    # Train the XGBoost model
    nrounds <- 100
    model <- xgboost(
        data = train_dmatrix,
        params = params,
        nrounds = nrounds,
        verbose = 3
    )
    
    # Predict on the test set
    predictions <- predict(model, test_dmatrix)
    
    # Calculate MAPE
    mape <- mean(abs((test_label - predictions) / test_label)) * 100
    
    # Return negative MAPE (since bayesOpt maximizes the score)
    return(list(Score = -mape))
}

## Defining range of hyperparameters

search_space <- list(
    max_depth = c(3L, 10L),
    eta = c(0.01, 0.3),
    gamma = c(0, 0.2),
    colsample_bytree = c(0.5, 1),
    min_child_weight = c(1, 10),
    subsample = c(0.7, 1)
)

# Perform Bayesian optimization using bayesOpt
bayes_opt_results <- bayesOpt(
    FUN = xgboost_optimization,
    bounds = search_space,
    initPoints = 10,  # Number of initial points
    iters.n = 30,  # Number of iterations
    verbose = TRUE
)

best_params <- getBestPars(bayes_opt_results)
best_params

# Train the final model using the best parameters
final_model <- xgboost(
    data = train_dmatrix,
    params = list(
        booster = "gbtree",
        objective = "reg:squarederror",  # Specify the regression objective
        max_depth = as.integer(best_params$max_depth),
        eta = best_params$eta,
        gamma = best_params$gamma,
        colsample_bytree = best_params$colsample_bytree,
        min_child_weight = best_params$min_child_weight,
        subsample = best_params$subsample,
        eval_metric = "mape"  # Use MAPE as the evaluation metric
    ),
    nrounds = 100,
    verbose = 0
)

# Predict on the test set
final_predictions <- predict(final_model, test_dmatrix)

# Calculate MAPE
final_mape <- mean(abs((test_label - final_predictions) / test_label)) * 100
print(paste("Final MAPE:", round(final_mape, 2)))

# Recursive Forecasting Function

recursive_forecast <- function(model, initial_data, n_steps) {
  predictions <- numeric(n_steps)
  current_data <- initial_data

  for (i in 1:n_steps) {
    dcurrent <- xgb.DMatrix(data = as.matrix(current_data))
    prediction <- predict(model, dcurrent)
    predictions[i] <- prediction
    
    # Update current_data with the new prediction
    new_row <- c(current_data[nrow(current_data), -ncol(current_data)], prediction)
    current_data <- rbind(current_data[-1,], new_row)
  }

  return(predictions)
}

# Use the last available data point for initial_data
initial_data <- as.matrix(vix_data[(nrow(vix_data)-2):nrow(vix_data), -1])  # Use the last 3 rows for lag features

# Forecast the next 12 steps
forecasted_values <- recursive_forecast(final_model, initial_data, 12)
print(forecasted_values)
