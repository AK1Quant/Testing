install.packages("ParBayesianOptimization")
library(ParBayesianOptimization)

# Define the objective function
xgboost_optimization <- function(max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample) {
    # Create a parameter list for the model
    params <- list(
        booster = "gbtree",
        objective = "reg:squarederror",
        max_depth = as.integer(max_depth),
        eta = eta,
        gamma = gamma,
        colsample_bytree = colsample_bytree,
        min_child_weight = min_child_weight,
        subsample = subsample,
        eval_metric = "rmse"
    )
    
    # Train the XGBoost model
    nrounds <- 100
    model <- xgboost(
        data = train_dmatrix,
        params = params,
        nrounds = nrounds,
        verbose = 0
    )
    
    # Predict on the test set
    predictions <- predict(model, test_dmatrix)
    
    # Calculate RMSE
    rmse <- sqrt(mean((test_label - predictions)^2))
    
    # Return negative RMSE to maximize using Bayesian optimization
    list(Score = -rmse, Pred = NA)
}

# Define the search space for hyperparameters
search_space <- list(
    max_depth = c(3L, 10L),
    eta = c(0.01, 0.3),
    gamma = c(0, 0.2),
    colsample_bytree = c(0.5, 1),
    min_child_weight = c(1, 10),
    subsample = c(0.7, 1)
)

# Run Bayesian optimization
bayesian_opt <- BayesianOptimization(
    FUN = xgboost_optimization,
    bounds = search_space,
    init_points = 5,  # Number of initial points
    n_iter = 10,  # Number of iterations
    acq = "ei"  # Acquisition function ("ei" for Expected Improvement)
)

# View the best parameters
print(bayesian_opt$Best_Par)

# Extract the best parameters
best_params <- bayesian_opt$Best_Par

# Train the final XGBoost model using the best parameters
final_model <- xgboost(
    data = train_dmatrix,
    params = list(
        booster = "gbtree",
        objective = "reg:squarederror",
        max_depth = as.integer(best_params$max_depth),
        eta = best_params$eta,
        gamma = best_params$gamma,
        colsample_bytree = best_params$colsample_bytree,
        min_child_weight = best_params$min_child_weight,
        subsample = best_params$subsample,
        eval_metric = "rmse"
    ),
    nrounds = 100,
    verbose = 0
)

# Predict on the test set
final_predictions <- predict(final_model, test_dmatrix)

# Calculate RMSE
final_rmse <- sqrt(mean((test_label - final_predictions)^2))
print(paste("Final RMSE:", round(final_rmse, 2)))

# Plot actual vs. predicted values
plot(test_data$Date, test_label, type = "l", col = "blue", ylab = "US VIX", xlab = "Date")
lines(test_data$Date, final_predictions, col = "red")
legend("topright", legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)

